{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31c91bfa",
   "metadata": {
    "papermill": {
     "duration": 0.003076,
     "end_time": "2023-03-08T08:00:47.019061",
     "exception": false,
     "start_time": "2023-03-08T08:00:47.015985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "68534287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T08:00:47.025319Z",
     "iopub.status.busy": "2023-03-08T08:00:47.025115Z",
     "iopub.status.idle": "2023-03-08T08:00:47.268927Z",
     "shell.execute_reply": "2023-03-08T08:00:47.268225Z"
    },
    "papermill": {
     "duration": 0.25046,
     "end_time": "2023-03-08T08:00:47.272487",
     "exception": false,
     "start_time": "2023-03-08T08:00:47.022027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "from conlleval import evaluate as conllevaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32a82ebc",
   "metadata": {},
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "90ad7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 3\n",
    "EARLY_STOPPING_LIMIT = 3\n",
    "DEBUG = True\n",
    "TRAINING_SAMPLES = 300\n",
    "DEV_SAMPLES = 100\n",
    "TEST_SAMPLES = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "760821b3",
   "metadata": {},
   "source": [
    "# Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0b3aa2f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "START = \"<START>\"\n",
    "STOP = \"<STOP>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5f68439",
   "metadata": {},
   "source": [
    "# Set Seed to make the training reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ad8334a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reproducible(seed: int=42) -> None:\n",
    "    \"\"\"Set seed to make the training reproducible.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "make_reproducible(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1c14128",
   "metadata": {},
   "source": [
    "# Read Train, Dev and Test NER Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0db28dbd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(filename) -> list:\n",
    "    \"\"\"\n",
    "    Reads the CoNLL 2003 data into an array of dictionaries (a dictionary for each data point).\n",
    "    :param filename: String\n",
    "    :return: Array of dictionaries.  Each dictionary has the format returned by the make_data_point function.\n",
    "    \"\"\"\n",
    "    data = list()\n",
    "    with open(filename, \"r\") as f:\n",
    "        sent = list()\n",
    "        for line in f.readlines():\n",
    "            if line.strip():\n",
    "                sent.append(line)\n",
    "            else:\n",
    "                data.append(make_data_point(sent))\n",
    "                sent = list()\n",
    "        data.append(make_data_point(sent))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_data_point(sent) -> dict:\n",
    "    \"\"\"\n",
    "        Creates a dictionary from String to an Array of Strings representing the data.  The dictionary items are:\n",
    "        dic['tokens'] = Tokens padded with <START> and <STOP>\n",
    "        dic['pos'] = POS tags padded with <START> and <STOP>\n",
    "        dic['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP> (but will not use)\n",
    "        dic['gold_tags'] = The gold tags padded with <START> and <STOP>\n",
    "    :param sent: String.  The input CoNLL format string\n",
    "    :return: Dict from String to Array of Strings.\n",
    "    \"\"\"\n",
    "    dic = dict()\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic[\"tokens\"] = [START] + [s[0] for s in sent] + [STOP]\n",
    "    dic[\"pos\"] = [START] + [s[1] for s in sent] + [STOP]\n",
    "    dic[\"NP_chunk\"] = [START] + [s[2] for s in sent] + [STOP]\n",
    "    dic[\"gold_tags\"] = [START] + [s[3] for s in sent] + [STOP]\n",
    "    return dic\n",
    "\n",
    "\n",
    "def read_gazetteer() -> list:\n",
    "    data = list()\n",
    "    with open(\"gazetteer.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            data += line.split()[1:]\n",
    "    return data\n",
    "\n",
    "datapath = \"./data\"\n",
    "gazetteer = read_gazetteer()\n",
    "\n",
    "train_data = read_data(os.path.join(datapath, \"ner.train\"))\n",
    "dev_data = read_data(os.path.join(datapath, \"ner.dev\"))\n",
    "test_data = read_data(os.path.join(datapath, \"ner.test\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a35ae28",
   "metadata": {},
   "source": [
    "# Tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8ef4a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = [\n",
    "    # Person (Begin, Inside)\n",
    "    \"B-PER\", \"I-PER\",\n",
    "    # Location (Begin, Inside)\n",
    "    \"B-LOC\", \"I-LOC\",\n",
    "    # Organization (Begin, Inside)\n",
    "    \"B-ORG\", \"I-ORG\",\n",
    "    # Miscelleneous (Begin, Inside)\n",
    "    \"B-MISC\", \"I-MISC\",\n",
    "    # Outside\n",
    "    \"O\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e520e4f9",
   "metadata": {},
   "source": [
    "# Debug on subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5016d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train_data = sample(train_data, TRAINING_SAMPLES)\n",
    "    dev_data = sample(dev_data, DEV_SAMPLES)\n",
    "    test_data = sample(test_data, TEST_SAMPLES)\n",
    "    \n",
    "training_size = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9bb10d4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureVector(object):\n",
    "    def __init__(self, fdict):\n",
    "        self.fdict = fdict\n",
    "\n",
    "    def times_plus_equal(self, scalar, v2) -> None:\n",
    "        \"\"\"\n",
    "        self += scalar * v2\n",
    "        :param scalar: Double\n",
    "        :param v2: FeatureVector\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for key, value in v2.fdict.items():\n",
    "            self.fdict[key] = scalar * value + self.fdict.get(key, 0)\n",
    "\n",
    "    def dot_product(self, v2) -> int:\n",
    "        \"\"\"\n",
    "        Computes the dot product between self and v2.  It is more efficient for v2 to be the smaller vector (fewer\n",
    "        non-zero entries).\n",
    "        :param v2: FeatureVector\n",
    "        :return: Int\n",
    "        \"\"\"\n",
    "        retval = 0\n",
    "        for key, value in v2.fdict.items():\n",
    "            retval += value * self.fdict.get(key, 0)\n",
    "        return retval\n",
    "\n",
    "    def square(self):\n",
    "        retvector = FeatureVector({})\n",
    "        for key, value in self.fdict.items():\n",
    "            val_sq = value * value\n",
    "            retvector.fdict[key] = val_sq\n",
    "        return retvector\n",
    "\n",
    "    def square_root(self):\n",
    "        retvector = FeatureVector({})\n",
    "        for key, value in self.fdict.items():\n",
    "            val_sq = math.sqrt(value)\n",
    "            retvector.fdict[key] = val_sq\n",
    "        return retvector\n",
    "\n",
    "    def divide(self, v2):\n",
    "        retvector = FeatureVector({})\n",
    "        for key, value in v2.fdict.items():\n",
    "            if value == 0:\n",
    "                retvector.fdict[key] = 0\n",
    "            else:\n",
    "                retvector.fdict[key] = self.fdict.get(key, 0) / value\n",
    "        return retvector\n",
    "\n",
    "    def write_to_file(self, filename) -> None:\n",
    "        \"\"\"\n",
    "        Writes the feature vector to a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\" \n",
    "        print(\"Writing to \" + filename)\n",
    "        path = os.path.join(\"./reports\", filename)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            features = [(k, v) for k, v in self.fdict.items()]\n",
    "            features.sort(key=lambda feature: feature[1], reverse=True)\n",
    "            for key, value in features:\n",
    "                f.write(f\"{key} {value}\\n\")\n",
    "\n",
    "    def read_from_file(self, filename) -> None:\n",
    "        \"\"\"\n",
    "        Reads a feature vector from a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.fdict = dict()\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                txt = line.split()\n",
    "                self.fdict[txt[0]] = float(txt[1])\n",
    "\n",
    "\n",
    "class Features(object):\n",
    "    def __init__(self, inputs, feature_names):\n",
    "        \"\"\"\n",
    "        Creates a Features object\n",
    "        :param inputs: Dictionary from String to an Array of Strings.\n",
    "            Created in the make_data_point function.\n",
    "            inputs['tokens'] = Tokens padded with <START> and <STOP>\n",
    "            inputs['pos'] = POS tags padded with <START> and <STOP>\n",
    "            inputs['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
    "            inputs['gold_tags'] = DON'T USE! The gold tags padded with <START> and <STOP>\n",
    "        :param feature_names: Array of Strings.  The list of features to compute.\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def compute_features(self, cur_tag, pre_tag, i):\n",
    "        \"\"\"\n",
    "        Computes the local features for the current tag, the previous tag, and position i\n",
    "        :param cur_tag: String.  The current tag.\n",
    "        :param pre_tag: String.  The previous tag.\n",
    "        :param i: Int. The position\n",
    "        :return: FeatureVector\n",
    "        \"\"\"\n",
    "        feats = FeatureVector({})\n",
    "        cur_word = self.inputs[\"tokens\"][i]\n",
    "        pos_tag = self.inputs[\"pos\"][i]\n",
    "        is_last = len(self.inputs[\"tokens\"]) - 1 == i\n",
    "        # Feature-1: Current word(Wi)\n",
    "        if \"current_word\" in self.feature_names:\n",
    "            key = f\"Wi={cur_word}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        # Feature-2: Previous Tag(Ti-1)\n",
    "        if \"prev_tag\" in self.feature_names:\n",
    "            key = f\"Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        # Feature-3: Lowercased Word(Oi)\n",
    "        if \"lowercase\" in self.feature_names:\n",
    "            key = f\"Oi={cur_word.lower()}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        # Feature-4: Current POS Tag(Pi)\n",
    "        if \"pos_tag\" in self.feature_names:\n",
    "            key = f\"Pi={pos_tag}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        # Feature-5: Shape of Current Word(Si)\n",
    "        if \"word_shape\" in self.feature_names:\n",
    "            word_shape = get_word_shape(cur_word)\n",
    "            key = f\"Si={word_shape}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        # Feature-6: (1-4 for prev + for next)\n",
    "        if \"feats_prev_and_next\" in self.feature_names:\n",
    "            prev_word = self.inputs[\"tokens\"][i - 1]\n",
    "            prev_pos = self.inputs[\"pos\"][i - 1]\n",
    "            prev_1 = f\"Wi-1={prev_word}+Ti={cur_tag}\"\n",
    "            prev_3 = f\"Oi-1={prev_word.lower()}+Ti={cur_tag}\"\n",
    "            prev_4 = f\"Pi-1={prev_pos}+Ti={cur_tag}\"\n",
    "            add_features(feats, prev_1)\n",
    "            add_features(feats, prev_3)\n",
    "            add_features(feats, prev_4)\n",
    "            if not is_last:\n",
    "                next_word = self.inputs[\"tokens\"][i + 1]\n",
    "                next_pos = self.inputs[\"pos\"][i + 1]\n",
    "                next_1 = f\"Wi+1={next_word}+Ti={cur_tag}\"\n",
    "                next_3 = f\"Oi+1={next_word.lower()}+Ti={cur_tag}\"\n",
    "                next_4 = f\"Pi+1={next_pos}+Ti={cur_tag}\"\n",
    "                add_features(feats, next_1)\n",
    "                add_features(feats, next_3)\n",
    "                add_features(feats, next_4)\n",
    "        # Feature-7: 1,3,4 conjoined with Previous Tag (pre_tag)\n",
    "        if \"feat_conjoined\" in self.feature_names:\n",
    "            conjoined_1 = f\"Wi={cur_word}+Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            conjoined_3 = f\"Oi={cur_word.lower()}+Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            conjoined_4 = f\"Pi={pos_tag}+Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            add_features(feats, conjoined_1)\n",
    "            add_features(feats, conjoined_3)\n",
    "            add_features(feats, conjoined_4)\n",
    "        # Feature-8: Prefix for Current word with lenhth k where k=1,2,3,4\n",
    "        if \"prefix_k\" in self.feature_names:\n",
    "            for k in range(4):\n",
    "                if k > len(cur_word):\n",
    "                    break\n",
    "                prefix = cur_word[: k + 1]\n",
    "                key = f\"PREi={prefix}+Ti={cur_tag}\"\n",
    "                add_features(feats, key)\n",
    "        # Feature-9: Gazetteer (GAZi)\n",
    "        if \"gazetteer\" in self.feature_names:\n",
    "            key = f\"GAZi={is_gazetteer(cur_word)}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        # Feature-10: Is capital (CAPi)\n",
    "        if \"capital\" in self.feature_names:\n",
    "            key = f\"CAPi={is_capital(cur_word)}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        # Feature-11: Position of the current word (indexed from 1)\n",
    "        if \"position\" in self.feature_names:\n",
    "            key = f\"POSi={i+1}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "        return feats\n",
    "\n",
    "\n",
    "def add_features(feats, key) -> None:\n",
    "    feats.times_plus_equal(1, FeatureVector({key: 1}))\n",
    "\n",
    "\n",
    "def get_word_shape(word):\n",
    "    shape = \"\"\n",
    "    for c in word:\n",
    "        shape += get_char_shape(c)\n",
    "    return shape\n",
    "\n",
    "\n",
    "def get_char_shape(char):\n",
    "    encoding = ord(char)\n",
    "    if encoding >= ord(\"a\") and encoding <= ord(\"z\"):\n",
    "        return \"a\"\n",
    "    if encoding >= ord(\"A\") and encoding <= ord(\"Z\"):\n",
    "        return \"A\"\n",
    "    if encoding >= ord(\"0\") and encoding <= ord(\"9\"):\n",
    "        return \"d\"\n",
    "    return char\n",
    "\n",
    "\n",
    "def is_gazetteer(word) -> str:\n",
    "    if word in gazetteer:\n",
    "        return \"True\"\n",
    "    return \"False\"\n",
    "\n",
    "\n",
    "def is_capital(word) -> str:\n",
    "    if len(word) == 0:\n",
    "        return \"False\"\n",
    "    c = ord(word[0])\n",
    "    if c >= ord(\"A\") and c <= ord(\"Z\"):\n",
    "        return \"True\"\n",
    "    return \"False\"\n",
    "\n",
    "\n",
    "def compute_features(tag_seq, input_length, features) -> FeatureVector:\n",
    "    \"\"\"\n",
    "    Compute f(xi, yi)\n",
    "    :param tag_seq: [tags] already padded with <START> and <STOP>\n",
    "    :param input_length: input length including the padding <START> and <STOP>\n",
    "    :param features: func from token index to FeatureVector\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    feats = FeatureVector({})\n",
    "    for i in range(1, input_length):\n",
    "        feats.times_plus_equal(1, features.compute_features(tag_seq[i], tag_seq[i - 1], i))\n",
    "    return feats\n",
    "\n",
    "    # Examples from class (from slides Jan 15, slide 18):\n",
    "    # x = will to fight\n",
    "    # y = NN TO VB\n",
    "    # features(x,y) =\n",
    "    #  {\"wi=will^yi=NN\": 1, // \"wi=\"+current_word+\"^yi=\"+current_tag\n",
    "    # \"yi-1=START^yi=NN\": 1,\n",
    "    # \"ti=to+^yi=TO\": 1,\n",
    "    # \"yi-1=NN+yi=TO\": 1,\n",
    "    # \"xi=fight^yi=VB\": 1,\n",
    "    # \"yi-1=TO^yi=VB\": 1}\n",
    "\n",
    "    # x = will to fight\n",
    "    # y = NN TO VBD\n",
    "    # features(x,y)=\n",
    "    # {\"wi=will^yi=NN\": 1,\n",
    "    # \"yi-1=START^yi=NN\": 1,\n",
    "    # \"ti=to+^yi=TO\": 1,\n",
    "    # \"yi-1=NN+yi=TO\": 1,\n",
    "    # \"xi=fight^yi=VBD\": 1,\n",
    "    # \"yi-1=TO^yi=VBD\": 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a983edb6",
   "metadata": {},
   "source": [
    "# Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9644c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited Feature Set -> Features 1-4\n",
    "limited_feature_set = [\"current_word\", \"prev_tag\", \"lowercase\", \"pos_tag\"]\n",
    "# Full Feature Set -> Features 1-4 + 5-11\n",
    "full_feature_set = limited_feature_set + [\"word_shape\", \"feats_prev_and_next\", \"feat_conjoined\", \"prefix_k\", \"gazetteer\", \"capital\", \"position\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5228a4e0",
   "metadata": {},
   "source": [
    "# Viterbi Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4a1f7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(viterbi_matrix, tagset, max_tag) -> list:\n",
    "    tags = list()\n",
    "    for k in reversed(range(len(viterbi_matrix))):\n",
    "        max_tag_idx = tagset.index(max_tag)\n",
    "        viterbi_list = viterbi_matrix[k]\n",
    "        max_tag, _ = viterbi_list[max_tag_idx]\n",
    "        tags = [max_tag] + tags\n",
    "    return tags\n",
    "\n",
    "\n",
    "def decode(input_len, tagset, score_func) -> list:\n",
    "    \"\"\"Viterbi Decoding to find the Best Tag Sequence\"\"\"\n",
    "    tags = list()\n",
    "    viterbi_matrix = list()\n",
    "    # Initial Step ,i.e. t=1\n",
    "    initial_list = list()\n",
    "    for tag in tagset:\n",
    "        score = score_func(tag, START, 1)\n",
    "        initial_list.append((START, score))\n",
    "    viterbi_matrix.append(initial_list)\n",
    "    # Recursion Step ,i.e. t=2,3,4,...\n",
    "    for t in range(2, input_len - 1):\n",
    "        viterbi_list = list()\n",
    "        for tag in tagset:\n",
    "            max_tag = None\n",
    "            max_score = float(\"-inf\")\n",
    "            for prev_tag in tagset:\n",
    "                last_viterbi_list = viterbi_matrix[t - 2]\n",
    "                prev_tag_idx = tagset.index(prev_tag)\n",
    "                last_score = last_viterbi_list[prev_tag_idx][1]\n",
    "                score = score_func(tag, prev_tag, t) + last_score\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    max_tag = prev_tag\n",
    "            viterbi_list.append((max_tag, score))\n",
    "        viterbi_matrix.append(viterbi_list)\n",
    "    # Termination Step\n",
    "    tags = [STOP] + tags\n",
    "    # Calculate the score for the max_tag\n",
    "    last_viterbi_list = list()\n",
    "    for tag in tagset:\n",
    "        stop_score = score_func(STOP, tag, input_len - 1)\n",
    "        prev_score = viterbi_matrix[-1][tagset.index(tag)][1]\n",
    "        score = stop_score + prev_score\n",
    "        last_viterbi_list.append((tag, score))\n",
    "    max_tag, _ = max(last_viterbi_list, key=lambda tuple: tuple[1])\n",
    "    tags = backtrack(viterbi_matrix, tagset, max_tag) + [max_tag] + tags\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "018e1d84",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(inputs, input_len, parameters, feature_names, tagset, score_func):\n",
    "    features = Features(inputs, feature_names)\n",
    "    gold_labels = inputs[\"gold_tags\"]\n",
    "    score = score_func(gold_labels, parameters, features)\n",
    "    return decode(input_len, tagset, score)\n",
    "\n",
    "\n",
    "def write_predictions(out_filename, all_inputs, parameters, feature_names, tagset, score_func):\n",
    "    \"\"\"\n",
    "    Writes the predictions on all_inputs to out_filename, in CoNLL 2003 evaluation format.\n",
    "    Each line is token, pos, NP_chuck_tag, gold_tag, predicted_tag (separated by spaces)\n",
    "    Sentences are separated by a newline\n",
    "    The file can be evaluated using the command: python conlleval.py < out_file\n",
    "    :param out_filename: filename of the output\n",
    "    :param all_inputs:\n",
    "    :param parameters:\n",
    "    :param feature_names:\n",
    "    :param tagset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred_dir = \"./predictions\"\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.makedirs(pred_dir)\n",
    "    path = os.path.join(pred_dir, out_filename)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for inputs in all_inputs:\n",
    "            input_len = len(inputs[\"tokens\"])\n",
    "            tag_seq = predict(inputs, input_len, parameters, feature_names, tagset, score_func)\n",
    "            for i, tag in enumerate(tag_seq[1:-1]):\n",
    "                f.write(\" \".join([inputs[\"tokens\"][i + 1], inputs[\"pos\"][i + 1], inputs[\"NP_chunk\"][i + 1], inputs[\"gold_tags\"][i + 1], tag]) + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def compute_score(tag_seq, input_length, score):\n",
    "    \"\"\"\n",
    "    Computes the total score of a tag sequence\n",
    "    :param tag_seq: Array of String of length input_length. The tag sequence including <START> and <STOP>\n",
    "    :param input_length: Int. input length including the padding <START> and <STOP>\n",
    "    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n",
    "        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    for i in range(1, input_length):\n",
    "        total_score += score(tag_seq[i], tag_seq[i - 1], i)\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def test_decoder() -> None:\n",
    "    # See https://classes.soe.ucsc.edu/nlp202/Winter21/assignments/A1_Debug_Example.pdf\n",
    "\n",
    "    tagset = [\"NN\", \"VB\"]  # make up our own tagset\n",
    "\n",
    "    def score_wrap(cur_tag, pre_tag, i):\n",
    "        retval = score(cur_tag, pre_tag, i)\n",
    "        print(\n",
    "            \"Score(\"\n",
    "            + cur_tag\n",
    "            + \",\"\n",
    "            + pre_tag\n",
    "            + \",\"\n",
    "            + str(i)\n",
    "            + \") returning \"\n",
    "            + str(retval)\n",
    "        )\n",
    "        return retval\n",
    "\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        if i == 0:\n",
    "            print(\n",
    "                \"ERROR: Don't call score for i = 0 (that points to <START>, with nothing before it)\"\n",
    "            )\n",
    "        if i == 1:\n",
    "            if pre_tag != \"<START>\":\n",
    "                print(\n",
    "                    \"ERROR: Previous tag should be <START> for i = 1. Previous tag = \"\n",
    "                    + pre_tag\n",
    "                )\n",
    "            if cur_tag == \"NN\":\n",
    "                return 6\n",
    "            if cur_tag == \"VB\":\n",
    "                return 4\n",
    "        if i == 2:\n",
    "            if cur_tag == \"NN\" and pre_tag == \"NN\":\n",
    "                return 4\n",
    "            if cur_tag == \"NN\" and pre_tag == \"VB\":\n",
    "                return 9\n",
    "            if cur_tag == \"VB\" and pre_tag == \"NN\":\n",
    "                return 5\n",
    "            if cur_tag == \"VB\" and pre_tag == \"VB\":\n",
    "                return 0\n",
    "        if i == 3:\n",
    "            if cur_tag != \"<STOP>\":\n",
    "                print(\n",
    "                    \"ERROR: Current tag at i = 3 should be <STOP>. Current tag = \"\n",
    "                    + cur_tag\n",
    "                )\n",
    "            if pre_tag == \"NN\":\n",
    "                return 1\n",
    "            if pre_tag == \"VB\":\n",
    "                return 1\n",
    "\n",
    "    predicted_tag_seq = decode(4, tagset, score_wrap)\n",
    "    print(\"Predicted tag sequence should be = <START> VB NN <STOP>\")\n",
    "    print(\"Predicted tag sequence = \" + \" \".join(predicted_tag_seq))\n",
    "    print(\n",
    "        \"Score of ['<START>','VB','NN','<STOP>'] = \"\n",
    "        + str(compute_score([\"<START>\", \"VB\", \"NN\", \"<STOP>\"], 4, score))\n",
    "    )\n",
    "    print(\"Max score should be = 14\")\n",
    "    print(\"Max score = \" + str(compute_score(predicted_tag_seq, 4, score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "06911b81",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimizer(update_func=\"ssgd\", l2_lambda=0.01):\n",
    "    sum_accumulator = FeatureVector({})\n",
    "    def adagrad(i, gradient, parameters, step_size):\n",
    "        grad = gradient(i)\n",
    "        sum_accumulator.times_plus_equal(1, grad.square())\n",
    "        parameters.times_plus_equal(-step_size, grad.divide(sum_accumulator.square_root()))\n",
    "        return parameters\n",
    "\n",
    "    def ssgd(i, gradient, parameters, step_size):\n",
    "        grad = gradient(i)\n",
    "        parameters.times_plus_equal(-step_size, grad)\n",
    "        return parameters\n",
    "\n",
    "    def l2_regularizer(i, gradient, parameters, step_size):\n",
    "        grad = gradient(i)\n",
    "        regularizer = FeatureVector({})\n",
    "        regularizer.times_plus_equal(l2_lambda, parameters)\n",
    "        parameters.times_plus_equal(-step_size, grad)\n",
    "        parameters.times_plus_equal(-step_size, regularizer)\n",
    "        return parameters\n",
    "\n",
    "    update = ssgd\n",
    "    if update_func == \"adagrad\":\n",
    "        update = adagrad\n",
    "    elif update_func == \"l2_regularizer\":\n",
    "        update = l2_regularizer\n",
    "\n",
    "    def optimizer_func(training_size, epochs, gradient, parameters, training_observer, step_size=1):\n",
    "        no_improve_count = 0\n",
    "        best_params = parameters\n",
    "        max_score = float(\"-inf\")\n",
    "        for epoch in range(epochs):\n",
    "            for i in tqdm(range(training_size), desc=\"Training...\", colour=\"red\"):\n",
    "                parameters = update(i, gradient, parameters, step_size)\n",
    "            # Evaluating on Dev Set\n",
    "            cur_score = training_observer(epoch, parameters)\n",
    "            cur_score = round(cur_score, 4)\n",
    "            print(f\"Epoch {epoch+1:02} -> F1-Score: {cur_score}\")\n",
    "            # Early Stopping\n",
    "            if cur_score >= max_score:\n",
    "                best_params = FeatureVector({})\n",
    "                best_params.times_plus_equal(1, parameters)\n",
    "                max_score = cur_score\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "            if no_improve_count > EARLY_STOPPING_LIMIT:\n",
    "                return best_params\n",
    "        return best_params\n",
    "    return optimizer_func\n",
    "\n",
    "\n",
    "def hamming_loss(loss_val=10, penalty=0):\n",
    "    def loss(gold, pred):\n",
    "        result = loss_val\n",
    "        if penalty > 0:\n",
    "            if gold != \"O\" and pred == \"O\":\n",
    "                result = penalty * result\n",
    "        return result if gold != pred else 0\n",
    "    return loss\n",
    "\n",
    "\n",
    "def svm_with_cost_func(cost_func):\n",
    "    def score(gold_labels, parameters, features):\n",
    "        return svm_score(gold_labels, parameters, features, cost_func=cost_func)\n",
    "    return score\n",
    "\n",
    "\n",
    "def perceptron_score(gold_labels, parameters, features):\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        return parameters.dot_product(\n",
    "            features.compute_features(cur_tag, pre_tag, i))\n",
    "    return score\n",
    "\n",
    "\n",
    "def svm_score(gold_labels, parameters, features, cost_func=hamming_loss()):\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        cost_val = cost_func(gold_labels[i], cur_tag)\n",
    "        cur_score = parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
    "        return cur_score + cost_val\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_gradient(data, feature_names, tagset, parameters, score_func):\n",
    "\n",
    "    def subgradient(i):\n",
    "        inputs = data[i]\n",
    "        input_len = len(inputs[\"tokens\"])\n",
    "        gold_labels = inputs[\"gold_tags\"]\n",
    "        features = Features(inputs, feature_names)\n",
    "        score = score_func(gold_labels, parameters, features)\n",
    "        # Viterbi Decoding to get the Predicted Tags\n",
    "        tags = decode(input_len, tagset, score)\n",
    "        fvector = compute_features(tags, input_len, features)\n",
    "        fvector.times_plus_equal(-1, compute_features(gold_labels, input_len, features))\n",
    "        return fvector\n",
    "\n",
    "    return subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ea10742c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data, parameters, feature_names, tagset, score_func, verbose=False):\n",
    "    \"\"\"Calculate Precision, Recall, and F1-Score on the given data.\"\"\"\n",
    "    all_gold_tags = list()\n",
    "    all_predicted_tags = list()\n",
    "    for inputs in tqdm(data, desc=\"Evaluating...\", colour=\"green\"):\n",
    "        all_gold_tags.extend(inputs[\"gold_tags\"][1:-1])\n",
    "        input_len = len(inputs[\"tokens\"])\n",
    "        all_predicted_tags.extend(predict(inputs,input_len,parameters,feature_names,tagset,score_func)[1:-1])\n",
    "    return conllevaluate(all_gold_tags, all_predicted_tags, verbose)\n",
    "\n",
    "\n",
    "def write_reports(reports, filename, columns) -> None:\n",
    "    report_map = dict()\n",
    "    for i, column in enumerate(columns):\n",
    "        values = list()\n",
    "        for report in reports:\n",
    "            value = report[i]\n",
    "            values.append(value)\n",
    "        report_map[column] = values\n",
    "    \n",
    "    reports_dir = \"./reports\"\n",
    "    if not os.path.exists(reports_dir):\n",
    "        os.makedirs(reports_dir)\n",
    "    path = os.path.join(reports_dir, filename)\n",
    "    report_df = pd.DataFrame(data=report_map)\n",
    "    report_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3d7c1dd0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(data, feature_names, tagset, epochs, optimizer, score_func=perceptron_score, step_size=1):\n",
    "    \"\"\"Train the Model on the given data.\"\"\"\n",
    "    parameters = FeatureVector({})\n",
    "    gradient = get_gradient(data, feature_names, tagset, parameters, score_func)\n",
    "    \n",
    "    def training_observer(epoch, parameters):\n",
    "        \"\"\"Evaluates the parameters on the Dev Set and returns the F1-Score.\"\"\"\n",
    "        (precision, recall, f1) = evaluate(dev_data, parameters, feature_names, tagset, score_func)\n",
    "        return f1\n",
    "    \n",
    "    return optimizer(training_size, epochs, gradient, parameters, training_observer, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f1824b25",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_and_test(parameters, feature_names, score, name) -> None:\n",
    "    report_cols = [\"precision\", \"recall\", \"f1\"]\n",
    "    # Dev Report Generation\n",
    "    report = evaluate(dev_data, parameters, feature_names, tagset, score)\n",
    "    dev_precision, dev_recall, dev_f1 = report\n",
    "    print(f\"DEV SET | Precision: {dev_precision:.4f} | Recall: {dev_recall:.4f} | F-1 Score: {dev_f1:.4f}\")\n",
    "    write_predictions(f\"{name}.dev.pred\", dev_data, parameters, feature_names, tagset, score)\n",
    "    write_reports([list(report)], f\"{name}.dev.report\", report_cols)\n",
    "    # Test Report Generation\n",
    "    report = evaluate(test_data, parameters, feature_names, tagset, score)\n",
    "    test_precision, test_recall, test_f1 = report\n",
    "    print(f\"TEST SET | Precision: {test_precision:.4f} | Recall: {test_recall:.4f} | F-1 Score: {test_f1:.4f}\")\n",
    "    write_predictions(f\"{name}.test.pred\", test_data, parameters, feature_names, tagset, score)\n",
    "    write_reports([list(report)], f\"{name}.test.report\", report_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76b2109d",
   "metadata": {},
   "source": [
    "# Header Printer Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e16abe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_printer(name: str) -> None:\n",
    "    print(\"*\" * 100)\n",
    "    print(name)\n",
    "    print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3f51247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_perceptron(feature_names, name, optimizer, write_params=False) -> None:\n",
    "    header_printer(name)\n",
    "    parameters = train(train_data, feature_names, tagset, epochs=EPOCHS, score_func=perceptron_score, optimizer=optimizer)\n",
    "    eval_and_test(parameters, feature_names, perceptron_score, name=name)\n",
    "    if write_params:\n",
    "        parameters.write_to_file(f\"{name}.parameters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b55bb73",
   "metadata": {},
   "source": [
    "# SVM Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "725a45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_config(): \n",
    "    step_sizes = [10, 50, 100]\n",
    "    lambdas = [5e-4, 1e-4, 1e-3]\n",
    "    cols = [\"step_size\", \n",
    "            \"l2_lambda\", \n",
    "            \"precision\", \n",
    "            \"recall\", \n",
    "            \"f1\"]\n",
    "    return step_sizes, lambdas, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c8519d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_svm(cost, feature_names, name) -> None:\n",
    "    header_printer(name)\n",
    "    config = svm_config()\n",
    "    score = svm_with_cost_func(cost)\n",
    "    best_f1_score = float(\"-inf\")\n",
    "    best_parameters = FeatureVector({})\n",
    "    reports = list()\n",
    "    for step_size in config[0]:\n",
    "        for l2_lambda in config[1]:\n",
    "            print(f\"Step-Size: {step_size} | Lambda: {l2_lambda}\")\n",
    "            parameters = train(train_data, feature_names, tagset, epochs=1, step_size=step_size, score_func=score, optimizer=optimizer(update_func=\"l2_regularizer\", l2_lambda=l2_lambda))\n",
    "            precision, recall, f1 = evaluate(dev_data, parameters, feature_names, tagset, score)\n",
    "            print(f\"Tuning Values: Precision: {precision:.4f} | Recall: {recall:.4f} | F-1 Score: {f1:.4f}\\n\")\n",
    "            reports.append([step_size, l2_lambda, precision, recall, f1])\n",
    "            if f1 > best_f1_score:\n",
    "                tuned_step_size = step_size\n",
    "                tuned_l2_lambda = l2_lambda\n",
    "                best_parameters = FeatureVector({})\n",
    "                best_parameters.times_plus_equal(1, parameters)\n",
    "                best_f1_score = f1\n",
    "    print(f\"\\nBEST CONFIG: Step-Size: {tuned_step_size} | Lambda: {tuned_l2_lambda}\\n\")\n",
    "    write_reports(reports, f\"{name}.tuning.report\", config[2])\n",
    "    eval_and_test(best_parameters, full_feature_set, score, name=name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c04d86f",
   "metadata": {},
   "source": [
    "# Limited Feature Set: Perceptron SSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3354b728",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "lfs_perceptron_ssgd\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [00:01<00:00, 292.89it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:00<00:00, 269.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 13.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [00:01<00:00, 288.88it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:00<00:00, 266.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 -> F1-Score: 44.8276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [00:01<00:00, 295.34it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:00<00:00, 278.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 -> F1-Score: 38.9189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:00<00:00, 272.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV SET | Precision: 50.6494 | Recall: 40.2062 | F-1 Score: 44.8276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:00<00:00, 297.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET | Precision: 45.0000 | Recall: 42.3529 | F-1 Score: 43.6364\n",
      "Writing to lfs_perceptron_ssgd.parameters\n"
     ]
    }
   ],
   "source": [
    "structured_perceptron(feature_names=limited_feature_set, name=\"lfs_perceptron_ssgd\", optimizer=optimizer(), write_params=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6865b1b",
   "metadata": {},
   "source": [
    "# Full Feature Set: Perceptron SSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f4569a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "ffs_perceptron_ssgd\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:36<00:00,  2.31it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 40.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:34<00:00,  2.33it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 -> F1-Score: 52.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:34<00:00,  2.33it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:23<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 -> F1-Score: 60.9626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:23<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV SET | Precision: 63.3333 | Recall: 58.7629 | F-1 Score: 60.9626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:21<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET | Precision: 59.0361 | Recall: 57.6471 | F-1 Score: 58.3333\n",
      "Writing to ffs_perceptron_ssgd.parameters\n"
     ]
    }
   ],
   "source": [
    "structured_perceptron(feature_names=full_feature_set, name=\"ffs_perceptron_ssgd\", optimizer=optimizer(), write_params=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93b2311e",
   "metadata": {},
   "source": [
    "# Full Feature Set: Perceptron Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "27d89a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "ffs_perceptron_adagrad\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:39<00:00,  2.27it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:23<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 57.2917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:41<00:00,  2.26it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:23<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 -> F1-Score: 59.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:41<00:00,  2.26it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:23<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 -> F1-Score: 67.3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV SET | Precision: 68.8172 | Recall: 65.9794 | F-1 Score: 67.3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:21<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET | Precision: 65.8824 | Recall: 65.8824 | F-1 Score: 65.8824\n"
     ]
    }
   ],
   "source": [
    "structured_perceptron(feature_names=full_feature_set, name=\"ffs_perceptron_adagrad\", optimizer=optimizer(update_func=\"adagrad\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "395f6690",
   "metadata": {},
   "source": [
    "# Full Feature Set: SVM SSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ed297d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "ffs_svm_ssgd\n",
      "****************************************************************************************************\n",
      "Step-Size: 10 | Lambda: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|\u001b[31m█▉        \u001b[0m| 59/300 [00:25<01:45,  2.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m structured_svm(hamming_loss(), feature_names\u001b[39m=\u001b[39mfull_feature_set, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mffs_svm_ssgd\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m l2_lambda \u001b[39min\u001b[39;00m config[\u001b[39m1\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStep-Size: \u001b[39m\u001b[39m{\u001b[39;00mstep_size\u001b[39m}\u001b[39;00m\u001b[39m | Lambda: \u001b[39m\u001b[39m{\u001b[39;00ml2_lambda\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     parameters \u001b[39m=\u001b[39m train(train_data, feature_names, tagset, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, step_size\u001b[39m=\u001b[39;49mstep_size, score_func\u001b[39m=\u001b[39;49mscore, optimizer\u001b[39m=\u001b[39;49moptimizer(update_func\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ml2_regularizer\u001b[39;49m\u001b[39m\"\u001b[39;49m, l2_lambda\u001b[39m=\u001b[39;49ml2_lambda))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     precision, recall, f1 \u001b[39m=\u001b[39m evaluate(dev_data, parameters, feature_names, tagset, score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTuning Values: Precision: \u001b[39m\u001b[39m{\u001b[39;00mprecision\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Recall: \u001b[39m\u001b[39m{\u001b[39;00mrecall\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | F-1 Score: \u001b[39m\u001b[39m{\u001b[39;00mf1\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     (precision, recall, f1) \u001b[39m=\u001b[39m evaluate(dev_data, parameters, feature_names, tagset, score_func)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f1\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer(training_size, epochs, gradient, parameters, training_observer, step_size\u001b[39m=\u001b[39;49mstep_size)\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(training_size), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining...\u001b[39m\u001b[39m\"\u001b[39m, colour\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mred\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         parameters \u001b[39m=\u001b[39m update(i, gradient, parameters, step_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m# Evaluating on Dev Set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     cur_score \u001b[39m=\u001b[39m training_observer(epoch, parameters)\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39ml2_regularizer\u001b[39m(i, gradient, parameters, step_size):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     grad \u001b[39m=\u001b[39m gradient(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     regularizer \u001b[39m=\u001b[39m FeatureVector({})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     regularizer\u001b[39m.\u001b[39mtimes_plus_equal(l2_lambda, parameters)\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m score \u001b[39m=\u001b[39m score_func(gold_labels, parameters, features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# Viterbi Decoding to get the Predicted Tags\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m tags \u001b[39m=\u001b[39m decode(input_len, tagset, score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m fvector \u001b[39m=\u001b[39m compute_features(tags, input_len, features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m fvector\u001b[39m.\u001b[39mtimes_plus_equal(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, compute_features(gold_labels, input_len, features))\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m prev_tag_idx \u001b[39m=\u001b[39m tagset\u001b[39m.\u001b[39mindex(prev_tag)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m last_score \u001b[39m=\u001b[39m last_viterbi_list[prev_tag_idx][\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m score \u001b[39m=\u001b[39m score_func(tag, prev_tag, t) \u001b[39m+\u001b[39m last_score\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m max_score:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     max_score \u001b[39m=\u001b[39m score\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore\u001b[39m(cur_tag, pre_tag, i):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     cost_val \u001b[39m=\u001b[39m cost_func(gold_labels[i], cur_tag)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     cur_score \u001b[39m=\u001b[39m parameters\u001b[39m.\u001b[39mdot_product(features\u001b[39m.\u001b[39;49mcompute_features(cur_tag, pre_tag, i))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cur_score \u001b[39m+\u001b[39m cost_val\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m# Feature-9: Gazetteer (GAZi)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mgazetteer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGAZi=\u001b[39m\u001b[39m{\u001b[39;00mis_gazetteer(cur_word)\u001b[39m}\u001b[39;00m\u001b[39m+Ti=\u001b[39m\u001b[39m{\u001b[39;00mcur_tag\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m     add_features(feats, key)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39m# Feature-10: Is capital (CAPi)\u001b[39;00m\n",
      "\u001b[1;32m/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb Cell 38\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_gazetteer\u001b[39m(word) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m gazetteer:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kushagraseth/Documents/Repositories/nlp-202/hw3/A3.ipynb#X42sZmlsZQ%3D%3D?line=199'>200</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "structured_svm(hamming_loss(), feature_names=full_feature_set, name=\"ffs_svm_ssgd\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "275ba1ae",
   "metadata": {},
   "source": [
    "# Full Feature Set: Modified SVM SSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e58a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "ffs_modified_svm_ssgd\n",
      "****************************************************************************************************\n",
      "Step-Size: 10 | Lambda: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:30<00:00,  2.37it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 18.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 23.8095 | Recall: 15.4639 | F-1 Score: 18.7500\n",
      "\n",
      "Step-Size: 10 | Lambda: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:31<00:00,  2.36it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 43.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 49.3333 | Recall: 38.1443 | F-1 Score: 43.0233\n",
      "\n",
      "Step-Size: 10 | Lambda: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:31<00:00,  2.36it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 16.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 19.4444 | Recall: 14.4330 | F-1 Score: 16.5680\n",
      "\n",
      "Step-Size: 50 | Lambda: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:30<00:00,  2.37it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 11.6883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 15.7895 | Recall: 9.2784 | F-1 Score: 11.6883\n",
      "\n",
      "Step-Size: 50 | Lambda: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:33<00:00,  2.35it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:21<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 22.3776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:21<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 34.7826 | Recall: 16.4948 | F-1 Score: 22.3776\n",
      "\n",
      "Step-Size: 50 | Lambda: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:30<00:00,  2.38it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 6.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 12.1212 | Recall: 4.1237 | F-1 Score: 6.1538\n",
      "\n",
      "Step-Size: 100 | Lambda: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:28<00:00,  2.40it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 15.0943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 19.3548 | Recall: 12.3711 | F-1 Score: 15.0943\n",
      "\n",
      "Step-Size: 100 | Lambda: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:32<00:00,  2.35it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 11.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 60.0000 | Recall: 6.1856 | F-1 Score: 11.2150\n",
      "\n",
      "Step-Size: 100 | Lambda: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|\u001b[31m██████████\u001b[0m| 500/500 [03:34<00:00,  2.34it/s]\n",
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 -> F1-Score: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Values: Precision: 100.0000 | Recall: 3.0928 | F-1 Score: 6.0000\n",
      "\n",
      "\n",
      "BEST CONFIG: Step-Size: 10 | Lambda: 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:22<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV SET | Precision: 49.3333 | Recall: 38.1443 | F-1 Score: 43.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|\u001b[32m██████████\u001b[0m| 50/50 [00:21<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET | Precision: 48.5714 | Recall: 40.0000 | F-1 Score: 43.8710\n"
     ]
    }
   ],
   "source": [
    "structured_svm(hamming_loss(penalty=30), feature_names=full_feature_set, name=\"ffs_modified_svm_ssgd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.205706,
   "end_time": "2023-03-08T08:00:56.333603",
   "environment_variables": {},
   "exception": null,
   "input_path": "a3.ipynb",
   "output_path": "a3.ipynb",
   "parameters": {
    "epoch": 5
   },
   "start_time": "2023-03-08T08:00:46.127897",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
