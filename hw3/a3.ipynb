{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import sample\n",
    "from conlleval import evaluate as conllevaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"<START>\"\n",
    "STOP = \"<STOP>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Reads the CoNLL 2003 data into an array of dictionaries (a dictionary for each data point).\n",
    "    :param filename: String\n",
    "    :return: Array of dictionaries.  Each dictionary has the format returned by the make_data_point function.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        sent = []\n",
    "        for line in f.readlines():\n",
    "            if line.strip():\n",
    "                sent.append(line)\n",
    "            else:\n",
    "                data.append(make_data_point(sent))\n",
    "                sent = []\n",
    "        data.append(make_data_point(sent))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_data_point(sent):\n",
    "    \"\"\"\n",
    "        Creates a dictionary from String to an Array of Strings representing the data.  The dictionary items are:\n",
    "        dic['tokens'] = Tokens padded with <START> and <STOP>\n",
    "        dic['pos'] = POS tags padded with <START> and <STOP>\n",
    "        dic['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP> (but will not use)\n",
    "        dic['gold_tags'] = The gold tags padded with <START> and <STOP>\n",
    "    :param sent: String.  The input CoNLL format string\n",
    "    :return: Dict from String to Array of Strings.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic[\"tokens\"] = [\"<START>\"] + [s[0] for s in sent] + [\"<STOP>\"]\n",
    "    dic[\"pos\"] = [\"<START>\"] + [s[1] for s in sent] + [\"<STOP>\"]\n",
    "    dic[\"NP_chunk\"] = [\"<START>\"] + [s[2] for s in sent] + [\"<STOP>\"]\n",
    "    dic[\"gold_tags\"] = [\"<START>\"] + [s[3] for s in sent] + [\"<STOP>\"]\n",
    "    return dic\n",
    "\n",
    "\n",
    "def read_gazetteer():\n",
    "    data = []\n",
    "    with open(\"gazetteer.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            data += line.split()[1:]\n",
    "    return data\n",
    "\n",
    "\n",
    "gazetteer = read_gazetteer()\n",
    "sample_num = 500\n",
    "train_data = read_data(\"ner.train\")\n",
    "dev_data = sample(read_data(\"ner.dev\"), 150)\n",
    "test_data = sample(read_data(\"ner.test\"), 150)\n",
    "tagset = [\n",
    "    \"B-PER\",\n",
    "    \"B-LOC\",\n",
    "    \"B-ORG\",\n",
    "    \"B-MISC\",\n",
    "    \"I-PER\",\n",
    "    \"I-LOC\",\n",
    "    \"I-ORG\",\n",
    "    \"I-MISC\",\n",
    "    \"O\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVector(object):\n",
    "    def __init__(self, fdict):\n",
    "        # mapping for features\n",
    "        self.fdict = fdict\n",
    "\n",
    "    def times_plus_equal(self, scalar, v2):\n",
    "        \"\"\"\n",
    "        self += scalar * v2\n",
    "        :param scalar: Double\n",
    "        :param v2: FeatureVector\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # update current feature vector given a vector v2\n",
    "        for key, value in v2.fdict.items():\n",
    "            self.fdict[key] = scalar * value + self.fdict.get(key, 0)\n",
    "\n",
    "    def dot_product(self, v2):\n",
    "        \"\"\"\n",
    "        Computes the dot product between self and v2.  It is more efficient for v2 to be the smaller vector (fewer\n",
    "        non-zero entries).\n",
    "        :param v2: FeatureVector\n",
    "        :return: Int\n",
    "        \"\"\"\n",
    "        # dot product with another feature vector\n",
    "        retval = 0\n",
    "        for key, value in v2.fdict.items():\n",
    "            retval += value * self.fdict.get(key, 0)\n",
    "        return retval\n",
    "\n",
    "    def square(self):\n",
    "        retvector = FeatureVector({})\n",
    "        for key, value in self.fdict.items():\n",
    "            val_sq = value * value\n",
    "            retvector.fdict[key] = val_sq\n",
    "        return retvector\n",
    "\n",
    "    def square_root(self):\n",
    "        retvector = FeatureVector({})\n",
    "        for key, value in self.fdict.items():\n",
    "            val_sq = math.sqrt(value)\n",
    "            retvector.fdict[key] = val_sq\n",
    "        return retvector\n",
    "\n",
    "    def divide(self, v2):\n",
    "        retvector = FeatureVector({})\n",
    "        for key, value in v2.fdict.items():\n",
    "            if value == 0:\n",
    "                retvector.fdict[key] = 0\n",
    "            else:\n",
    "                retvector.fdict[key] = self.fdict.get(key, 0) / value\n",
    "        return retvector\n",
    "\n",
    "    def write_to_file(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the feature vector to a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Writing to \" + filename)\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            features = [(k, v) for k, v in self.fdict.items()]\n",
    "            features.sort(key=lambda feature: feature[1], reverse=True)\n",
    "            for key, value in features:\n",
    "                f.write(\"{} {}\\n\".format(key, value))\n",
    "\n",
    "    def read_from_file(self, filename):\n",
    "        \"\"\"\n",
    "        Reads a feature vector from a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.fdict = {}\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                txt = line.split()\n",
    "                self.fdict[txt[0]] = float(txt[1])\n",
    "\n",
    "\n",
    "class Features(object):\n",
    "    def __init__(self, inputs, feature_names):\n",
    "        \"\"\"\n",
    "        Creates a Features object\n",
    "        :param inputs: Dictionary from String to an Array of Strings.\n",
    "            Created in the make_data_point function.\n",
    "            inputs['tokens'] = Tokens padded with <START> and <STOP>\n",
    "            inputs['pos'] = POS tags padded with <START> and <STOP>\n",
    "            inputs['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
    "            inputs['gold_tags'] = DON'T USE! The gold tags padded with <START> and <STOP>\n",
    "        :param feature_names: Array of Strings.  The list of features to compute.\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def compute_features(self, cur_tag, pre_tag, i):\n",
    "        \"\"\"\n",
    "        Computes the local features for the current tag, the previous tag, and position i\n",
    "        :param cur_tag: String.  The current tag.\n",
    "        :param pre_tag: String.  The previous tag.\n",
    "        :param i: Int. The position\n",
    "        :return: FeatureVector\n",
    "        \"\"\"\n",
    "        feats = FeatureVector({})\n",
    "        cur_word = self.inputs[\"tokens\"][i]\n",
    "        pos_tag = self.inputs[\"pos\"][i]\n",
    "        is_last = len(self.inputs[\"tokens\"]) - 1 == i\n",
    "\n",
    "        # 1 cur word\n",
    "        # i.e. Wi=France+Ti=I-LOC 1.0\n",
    "        if \"current_word\" in self.feature_names:\n",
    "            key = f\"Wi={cur_word}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        # 2 prev tag\n",
    "        # i.e. Ti-1=<START>+Ti=I-LOC 1.0\n",
    "        if \"prev_tag\" in self.feature_names:\n",
    "            key = f\"Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        # 3 lowercased\n",
    "        # i.e. Oi=france+Ti=I-LOC 1.0\n",
    "        if \"lowercase\" in self.feature_names:\n",
    "            key = f\"Oi={cur_word.lower()}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        # 4 cur pos\n",
    "        # i.e. Pi=NNP+Ti=I-LOC 1.0\n",
    "        if \"pos_tag\" in self.feature_names:\n",
    "            key = f\"Pi={pos_tag}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        # 5 shape of word\n",
    "        # i.e. Si=Aaaaaa+Ti=I-LOC\n",
    "        if \"word_shape\" in self.feature_names:\n",
    "            word_shape = get_word_shape(cur_word)\n",
    "            key = f\"Si={word_shape}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        # 6 (1-4 for prev + for next)\n",
    "        # i.e. Wi-1=<START>+Ti=I-LOC 1.0\n",
    "        # i.e. Pi-1=<START>+Ti=I-LOC 1.0\n",
    "        # i.e. Wi+1=and+Ti=I-LOC 1.0\n",
    "\n",
    "        if \"feats_prev_and_next\" in self.feature_names:\n",
    "            prev_word = self.inputs[\"tokens\"][i - 1]\n",
    "            prev_pos = self.inputs[\"pos\"][i - 1]\n",
    "            prev_1 = f\"Wi-1={prev_word}+Ti={cur_tag}\"\n",
    "            prev_3 = f\"Oi-1={prev_word.lower()}+Ti={cur_tag}\"\n",
    "            prev_4 = f\"Pi-1={prev_pos}+Ti={cur_tag}\"\n",
    "            add_features(feats, prev_1)\n",
    "            add_features(feats, prev_3)\n",
    "            add_features(feats, prev_4)\n",
    "\n",
    "            if not is_last:\n",
    "                next_word = self.inputs[\"tokens\"][i + 1]\n",
    "                next_pos = self.inputs[\"pos\"][i + 1]\n",
    "                next_1 = f\"Wi+1={next_word}+Ti={cur_tag}\"\n",
    "                next_3 = f\"Oi+1={next_word.lower()}+Ti={cur_tag}\"\n",
    "                next_4 = f\"Pi+1={next_pos}+Ti={cur_tag}\"\n",
    "                add_features(feats, next_1)\n",
    "                add_features(feats, next_3)\n",
    "                add_features(feats, next_4)\n",
    "\n",
    "        # 7 1,3,4 conjoined with pre_tag\n",
    "        # i.e. Wi+1=and+Ti-1=<START>+Ti=I-LOC 1.0\n",
    "        if \"feat_conjoined\" in self.feature_names:\n",
    "            conjoined_1 = f\"Wi={cur_word}+Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            conjoined_3 = f\"Oi={cur_word.lower()}+Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            conjoined_4 = f\"Pi={pos_tag}+Ti-1={pre_tag}+Ti={cur_tag}\"\n",
    "            add_features(feats, conjoined_1)\n",
    "            add_features(feats, conjoined_3)\n",
    "            add_features(feats, conjoined_4)\n",
    "\n",
    "        # 8 k=1,2,3,4 prefix\n",
    "        # i.e. PREi=Fr+Ti=I-LOC 1.0\n",
    "        # i.e. PREi=Fra+Ti=I-LOC 1.0\n",
    "        if \"prefix_k\" in self.feature_names:\n",
    "            for k in range(4):\n",
    "                if k > len(cur_word):\n",
    "                    break\n",
    "                prefix = cur_word[: k + 1]\n",
    "                key = f\"PREi={prefix}+Ti={cur_tag}\"\n",
    "                add_features(feats, key)\n",
    "\n",
    "        # 9 gazetteer\n",
    "        # i.e. GAZi=True+Ti=I-LOC 1.0\n",
    "        if \"gazetteer\" in self.feature_names:\n",
    "            key = f\"GAZi={is_gazetteer(cur_word)}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        # 10 is capital\n",
    "        # i.e. CAPi=True+Ti=I-LOC 1.0\n",
    "        if \"capital\" in self.feature_names:\n",
    "            key = f\"CAPi={is_capital(cur_word)}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        # 11 position\n",
    "        # i.e. POSi=1+Ti=I-LOC 1.0\n",
    "        if \"position\" in self.feature_names:\n",
    "            key = f\"POSi={i+1}+Ti={cur_tag}\"\n",
    "            add_features(feats, key)\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "def add_features(feats, key):\n",
    "    feats.times_plus_equal(\n",
    "        1,\n",
    "        FeatureVector(\n",
    "            {key: 1},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_word_shape(word):\n",
    "    shape = \"\"\n",
    "    for c in word:\n",
    "        shape += get_char_shape(c)\n",
    "    return shape\n",
    "\n",
    "\n",
    "def get_char_shape(char):\n",
    "    encoding = ord(char)\n",
    "    if encoding >= ord(\"a\") and encoding <= ord(\"z\"):\n",
    "        return \"a\"\n",
    "\n",
    "    if encoding >= ord(\"A\") and encoding <= ord(\"Z\"):\n",
    "        return \"A\"\n",
    "\n",
    "    if encoding >= ord(\"0\") and encoding <= ord(\"9\"):\n",
    "        return \"d\"\n",
    "\n",
    "    return char\n",
    "\n",
    "\n",
    "def is_gazetteer(word):\n",
    "    if word in gazetteer:\n",
    "        return \"True\"\n",
    "    return \"False\"\n",
    "\n",
    "\n",
    "def is_capital(word):\n",
    "    if len(word) == 0:\n",
    "        return \"False\"\n",
    "    c = ord(word[0])\n",
    "    if c >= ord(\"A\") and c <= ord(\"Z\"):\n",
    "        return \"True\"\n",
    "    return \"False\"\n",
    "\n",
    "\n",
    "def compute_features(tag_seq, input_length, features):\n",
    "    \"\"\"\n",
    "    Compute f(xi, yi)\n",
    "    :param tag_seq: [tags] already padded with <START> and <STOP>\n",
    "    :param input_length: input length including the padding <START> and <STOP>\n",
    "    :param features: func from token index to FeatureVector\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # compute feature given sequence\n",
    "    feats = FeatureVector({})\n",
    "    for i in range(1, input_length):\n",
    "        feats.times_plus_equal(\n",
    "            1, features.compute_features(tag_seq[i], tag_seq[i - 1], i)\n",
    "        )\n",
    "    return feats\n",
    "\n",
    "    # Examples from class (from slides Jan 15, slide 18):\n",
    "    # x = will to fight\n",
    "    # y = NN TO VB\n",
    "    # features(x,y) =\n",
    "    #  {\"wi=will^yi=NN\": 1, // \"wi=\"+current_word+\"^yi=\"+current_tag\n",
    "    # \"yi-1=START^yi=NN\": 1,\n",
    "    # \"ti=to+^yi=TO\": 1,\n",
    "    # \"yi-1=NN+yi=TO\": 1,\n",
    "    # \"xi=fight^yi=VB\": 1,\n",
    "    # \"yi-1=TO^yi=VB\": 1}\n",
    "\n",
    "    # x = will to fight\n",
    "    # y = NN TO VBD\n",
    "    # features(x,y)=\n",
    "    # {\"wi=will^yi=NN\": 1,\n",
    "    # \"yi-1=START^yi=NN\": 1,\n",
    "    # \"ti=to+^yi=TO\": 1,\n",
    "    # \"yi-1=NN+yi=TO\": 1,\n",
    "    # \"xi=fight^yi=VBD\": 1,\n",
    "    # \"yi-1=TO^yi=VBD\": 1}\n",
    "\n",
    "\n",
    "feature_1_to_4 = [\"current_word\", \"prev_tag\", \"lowercase\", \"pos_tag\"]\n",
    "feature_full = feature_1_to_4 + [\n",
    "    \"word_shape\",\n",
    "    \"feats_prev_and_next\",\n",
    "    \"feat_conjoined\",\n",
    "    \"prefix_k\",\n",
    "    \"gazetteer\",\n",
    "    \"capital\",\n",
    "    \"position\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(viterbi_matrix, tagset, max_tag):\n",
    "    tags = []\n",
    "    for k in reversed(range(len(viterbi_matrix))):\n",
    "        last_max_tag_idx = tagset.index(max_tag)\n",
    "        viterbi_list = viterbi_matrix[k]\n",
    "        max_tag, _ = viterbi_list[last_max_tag_idx]\n",
    "        tags = [max_tag] + tags\n",
    "    return tags\n",
    "\n",
    "\n",
    "def decode(input_len, tagset, score_func):\n",
    "    \"\"\"Viterbi Decoding\n",
    "\n",
    "    Args:\n",
    "        input_len (int): input length\n",
    "        tagset (list): the list of all tags\n",
    "        score_func (func): score function\n",
    "\n",
    "    Returns:\n",
    "        tags (list): predicted tag sequence\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    viterbi_matrix = []\n",
    "\n",
    "    # initial step\n",
    "    initial_list = []\n",
    "    for tag in tagset:\n",
    "        score = score_func(tag, START, 1)\n",
    "        initial_list.append((START, score))\n",
    "    viterbi_matrix.append(initial_list)\n",
    "\n",
    "    # recursion step\n",
    "    for t in range(2, input_len - 1):\n",
    "\n",
    "        viterbi_list = []\n",
    "        for tag in tagset:\n",
    "            # max() and argmax()\n",
    "            max_tag = None\n",
    "            max_score = float(\"-inf\")\n",
    "            for prev_tag in tagset:\n",
    "                last_viterbi_list = viterbi_matrix[t - 2]\n",
    "                prev_tag_idx = tagset.index(prev_tag)\n",
    "                last_score = last_viterbi_list[prev_tag_idx][1]\n",
    "                score = score_func(tag, prev_tag, t) + last_score\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    max_tag = prev_tag\n",
    "            viterbi_list.append((max_tag, score))\n",
    "        viterbi_matrix.append(viterbi_list)\n",
    "    # termination step\n",
    "    tags = [STOP] + tags\n",
    "\n",
    "    # calculate the max tag\n",
    "    last_viterbi_list = []\n",
    "    for tag in tagset:\n",
    "        stop_score = score_func(STOP, tag, input_len - 1)\n",
    "        prev_score = viterbi_matrix[-1][tagset.index(tag)][1]\n",
    "        score = stop_score + prev_score\n",
    "        last_viterbi_list.append((tag, score))\n",
    "    max_tag, _ = max(last_viterbi_list, key=lambda tuple: tuple[1])\n",
    "\n",
    "    tags = backtrack(viterbi_matrix, tagset, max_tag) + [max_tag] + tags\n",
    "    return tags\n",
    "\n",
    "\n",
    "def predict(inputs, input_len, parameters, feature_names, tagset, score_func):\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs:\n",
    "    :param input_len:\n",
    "    :param parameters:\n",
    "    :param feature_names:\n",
    "    :param tagset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    features = Features(inputs, feature_names)\n",
    "    gold_labels = inputs[\"gold_tags\"]\n",
    "\n",
    "    score = score_func(gold_labels, parameters, features)\n",
    "\n",
    "    return decode(input_len, tagset, score)\n",
    "\n",
    "\n",
    "def write_predictions(\n",
    "    out_filename, all_inputs, parameters, feature_names, tagset, score_func\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes the predictions on all_inputs to out_filename, in CoNLL 2003 evaluation format.\n",
    "    Each line is token, pos, NP_chuck_tag, gold_tag, predicted_tag (separated by spaces)\n",
    "    Sentences are separated by a newline\n",
    "    The file can be evaluated using the command: python conlleval.py < out_file\n",
    "    :param out_filename: filename of the output\n",
    "    :param all_inputs:\n",
    "    :param parameters:\n",
    "    :param feature_names:\n",
    "    :param tagset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(out_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for inputs in all_inputs:\n",
    "            input_len = len(inputs[\"tokens\"])\n",
    "            tag_seq = predict(\n",
    "                inputs,\n",
    "                input_len,\n",
    "                parameters,\n",
    "                feature_names,\n",
    "                tagset,\n",
    "                score_func,\n",
    "            )\n",
    "            for i, tag in enumerate(\n",
    "                tag_seq[1:-1]\n",
    "            ):  # deletes <START> and <STOP>\n",
    "                f.write(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            inputs[\"tokens\"][i + 1],\n",
    "                            inputs[\"pos\"][i + 1],\n",
    "                            inputs[\"NP_chunk\"][i + 1],\n",
    "                            inputs[\"gold_tags\"][i + 1],\n",
    "                            tag,\n",
    "                        ]\n",
    "                    )\n",
    "                    + \"\\n\"\n",
    "                )  # i + 1 because of <START>\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def compute_score(tag_seq, input_length, score):\n",
    "    \"\"\"\n",
    "    Computes the total score of a tag sequence\n",
    "    :param tag_seq: Array of String of length input_length. The tag sequence including <START> and <STOP>\n",
    "    :param input_length: Int. input length including the padding <START> and <STOP>\n",
    "    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n",
    "        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    for i in range(1, input_length):\n",
    "        total_score += score(tag_seq[i], tag_seq[i - 1], i)\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def test_decoder():\n",
    "    # See https://classes.soe.ucsc.edu/nlp202/Winter21/assignments/A1_Debug_Example.pdf\n",
    "\n",
    "    tagset = [\"NN\", \"VB\"]  # make up our own tagset\n",
    "\n",
    "    def score_wrap(cur_tag, pre_tag, i):\n",
    "        retval = score(cur_tag, pre_tag, i)\n",
    "        print(\n",
    "            \"Score(\"\n",
    "            + cur_tag\n",
    "            + \",\"\n",
    "            + pre_tag\n",
    "            + \",\"\n",
    "            + str(i)\n",
    "            + \") returning \"\n",
    "            + str(retval)\n",
    "        )\n",
    "        return retval\n",
    "\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        if i == 0:\n",
    "            print(\n",
    "                \"ERROR: Don't call score for i = 0 (that points to <START>, with nothing before it)\"\n",
    "            )\n",
    "        if i == 1:\n",
    "            if pre_tag != \"<START>\":\n",
    "                print(\n",
    "                    \"ERROR: Previous tag should be <START> for i = 1. Previous tag = \"\n",
    "                    + pre_tag\n",
    "                )\n",
    "            if cur_tag == \"NN\":\n",
    "                return 6\n",
    "            if cur_tag == \"VB\":\n",
    "                return 4\n",
    "        if i == 2:\n",
    "            if cur_tag == \"NN\" and pre_tag == \"NN\":\n",
    "                return 4\n",
    "            if cur_tag == \"NN\" and pre_tag == \"VB\":\n",
    "                return 9\n",
    "            if cur_tag == \"VB\" and pre_tag == \"NN\":\n",
    "                return 5\n",
    "            if cur_tag == \"VB\" and pre_tag == \"VB\":\n",
    "                return 0\n",
    "        if i == 3:\n",
    "            if cur_tag != \"<STOP>\":\n",
    "                print(\n",
    "                    \"ERROR: Current tag at i = 3 should be <STOP>. Current tag = \"\n",
    "                    + cur_tag\n",
    "                )\n",
    "            if pre_tag == \"NN\":\n",
    "                return 1\n",
    "            if pre_tag == \"VB\":\n",
    "                return 1\n",
    "\n",
    "    predicted_tag_seq = decode(4, tagset, score_wrap)\n",
    "    print(\"Predicted tag sequence should be = <START> VB NN <STOP>\")\n",
    "    print(\"Predicted tag sequence = \" + \" \".join(predicted_tag_seq))\n",
    "    print(\n",
    "        \"Score of ['<START>','VB','NN','<STOP>'] = \"\n",
    "        + str(compute_score([\"<START>\", \"VB\", \"NN\", \"<STOP>\"], 4, score))\n",
    "    )\n",
    "    print(\"Max score should be = 14\")\n",
    "    print(\"Max score = \" + str(compute_score(predicted_tag_seq, 4, score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOP_NO_IMPROVE_LIMIT = 3\n",
    "\n",
    "\n",
    "def optimizer(update_func=\"ssgd\", l2_lambda=0.01):\n",
    "\n",
    "    # only for adagrad\n",
    "    # a feature vector for accumulated gradient square sum\n",
    "    accum_sum = FeatureVector({})\n",
    "\n",
    "    def adagrad(\n",
    "        i,\n",
    "        gradient,\n",
    "        parameters,\n",
    "        step_size,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        AdaGrad update\n",
    "        :param i: index of current instance\n",
    "        :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n",
    "        :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n",
    "        :param step_size: int. Learning rate, step size\n",
    "        :return: updated parameters\n",
    "        \"\"\"\n",
    "        # step_size / sqrt(accum_sum) * grad\n",
    "        # accum_sum = sum_t(grad_t**2)\n",
    "        grad = gradient(i)\n",
    "        accum_sum.times_plus_equal(1, grad.square())\n",
    "        parameters.times_plus_equal(\n",
    "            -step_size, grad.divide(accum_sum.square_root())\n",
    "        )\n",
    "        return parameters\n",
    "\n",
    "    def ssgd(i, gradient, parameters, step_size):\n",
    "        \"\"\"\n",
    "        Stochastic sub-gradient descent update\n",
    "        :param i: index of current instance\n",
    "        :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n",
    "        :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n",
    "        :param step_size: int. Learning rate, step size\n",
    "        :return: updated parameters\n",
    "        \"\"\"\n",
    "        # Look at the FeatureVector object.  You'll want to use the function times_plus_equal to update the parameters.\n",
    "        # gradient in feature vector class\n",
    "        grad = gradient(i)\n",
    "        # w − α g(x, y)\n",
    "        parameters.times_plus_equal(-step_size, grad)\n",
    "        return parameters\n",
    "\n",
    "    def l2_regularizer(i, gradient, parameters, step_size):\n",
    "        \"\"\"\n",
    "        Stochastic sub-gradient descent update with L2 regularizer\n",
    "        :param i: index of current instance\n",
    "        :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n",
    "        :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n",
    "        :param step_size: int. Learning rate, step size\n",
    "        :return: updated parameters\n",
    "        \"\"\"\n",
    "        # Look at the FeatureVector object.  You'll want to use the function times_plus_equal to update the parameters.\n",
    "        # gradient in feature vector class\n",
    "        grad = gradient(i)\n",
    "        #  learning for l2 regularizer: w − α g(x, y) − αλw\n",
    "        # λw\n",
    "        regularizer = FeatureVector({})\n",
    "        regularizer.times_plus_equal(l2_lambda, parameters)\n",
    "        # w − α g(x, y) − αλw\n",
    "        parameters.times_plus_equal(-step_size, grad)\n",
    "        parameters.times_plus_equal(-step_size, regularizer)\n",
    "        return parameters\n",
    "\n",
    "    update = ssgd\n",
    "    if update_func == \"adagrad\":\n",
    "        update = adagrad\n",
    "    elif update_func == \"l2_regularizer\":\n",
    "        update = l2_regularizer\n",
    "\n",
    "    def optimizer_func(\n",
    "        training_size,\n",
    "        epochs,\n",
    "        gradient,\n",
    "        parameters,\n",
    "        training_observer,\n",
    "        step_size=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Optimization Function (Based on Gradient Descent)\n",
    "        :param training_size: int. Number of examples in the training set\n",
    "        :param epochs: int. Number of epochs to run SGD for\n",
    "        :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n",
    "        :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n",
    "        :param training_observer: func that takes epoch and parameters.  You can call this function at the end of each\n",
    "            epoch to evaluate on a dev set and write out the model parameters for early stopping.\n",
    "        :param step_size: int. Learning rate, step size\n",
    "        :return: final parameters\n",
    "        \"\"\"\n",
    "\n",
    "        no_improve_count = 0\n",
    "\n",
    "        best_params = parameters\n",
    "        max_score = float(\"-inf\")\n",
    "\n",
    "        # go through every epochs\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # go through every training data\n",
    "            for i in tqdm(range(training_size), desc=\"Training...\", colour=\"red\"):\n",
    "                parameters = update(i, gradient, parameters, step_size)\n",
    "\n",
    "            # dev score\n",
    "            cur_score = training_observer(epoch, parameters)\n",
    "            print(f\"F-1 score at epoch {epoch}: {cur_score}\")\n",
    "\n",
    "            # updating best parameters\n",
    "            if cur_score >= max_score:\n",
    "                best_params = FeatureVector({})\n",
    "                best_params.times_plus_equal(1, parameters)\n",
    "                max_score = cur_score\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                # if no improvement\n",
    "                no_improve_count += 1\n",
    "\n",
    "            # if larger than tolerable no improvement times\n",
    "            if no_improve_count > EARLY_STOP_NO_IMPROVE_LIMIT:\n",
    "                # early stopping\n",
    "                return best_params\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    return optimizer_func\n",
    "\n",
    "\n",
    "def hamming_loss(loss_val=10, penalty=0):\n",
    "    \"\"\"\n",
    "    Modify the cost function to penalize mistakes three times more (penalty of 30) if the gold standard has a tag\n",
    "    that is not O but the candidate tag is O.\n",
    "\n",
    "    Args:\n",
    "        penalty (Int)\n",
    "    \"\"\"\n",
    "\n",
    "    def loss(gold, pred):\n",
    "        result = loss_val\n",
    "        if penalty > 0:\n",
    "            if gold != \"O\" and pred == \"O\":\n",
    "                result = penalty * result\n",
    "        return result if gold != pred else 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def svm_with_cost_func(cost_func):\n",
    "    def score(gold_labels, parameters, features):\n",
    "        return svm_score(\n",
    "            gold_labels, parameters, features, cost_func=cost_func\n",
    "        )\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def perceptron_score(gold_labels, parameters, features):\n",
    "    # score function given current tag and previous tag with the parameter\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        # w dot f(x, y')\n",
    "        return parameters.dot_product(\n",
    "            features.compute_features(cur_tag, pre_tag, i)\n",
    "        )\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def svm_score(gold_labels, parameters, features, cost_func=hamming_loss()):\n",
    "    # score function given current tag and previous tag with the parameter\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        # w dot f(x, y')\n",
    "        cost_val = cost_func(gold_labels[i], cur_tag)\n",
    "        cur_score = parameters.dot_product(\n",
    "            features.compute_features(cur_tag, pre_tag, i)\n",
    "        )\n",
    "        return cur_score + cost_val\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_gradient(data, feature_names, tagset, parameters, score_func):\n",
    "    data = sample(data, sample_num)\n",
    "\n",
    "    def subgradient(i):\n",
    "        \"\"\"\n",
    "        Computes the subgradient of the Perceptron loss for example i\n",
    "        :param i: Int\n",
    "        :return: FeatureVector\n",
    "        \"\"\"\n",
    "        # data point at i\n",
    "        inputs = data[i]\n",
    "        # get the token length\n",
    "        input_len = len(inputs[\"tokens\"])\n",
    "        # get the gold labels\n",
    "        gold_labels = inputs[\"gold_tags\"]\n",
    "        # get the features given feature names\n",
    "        features = Features(inputs, feature_names)\n",
    "        score = score_func(gold_labels, parameters, features)\n",
    "        # use viterbi algorithm for decoding the tags\n",
    "        tags = decode(input_len, tagset, score)\n",
    "        # print(tags, gold_labels)\n",
    "        # Add the predicted features\n",
    "        fvector = compute_features(tags, input_len, features)\n",
    "\n",
    "        # print(\"Input:\", inputs)  # helpful for debugging\n",
    "        # print(\"Predicted Feature Vector:\", fvector.fdict)\n",
    "        # print(\n",
    "        #     \"Predicted Score:\", parameters.dot_product(fvector)\n",
    "        # )  # compute_score(tags, input_len, score)\n",
    "\n",
    "        # Subtract the features for the gold labels\n",
    "        fvector.times_plus_equal(\n",
    "            -1, compute_features(gold_labels, input_len, features)\n",
    "        )\n",
    "        # print(\n",
    "        #     \"Gold Labels Feature Vector: \",\n",
    "        #     compute_features(gold_labels, input_len, features).fdict,\n",
    "        # )\n",
    "        # print(\n",
    "        #     \"Gold Labels Score:\",\n",
    "        #     parameters.dot_product(\n",
    "        #         compute_features(gold_labels, input_len, features)\n",
    "        #     ),\n",
    "        # )\n",
    "        # return the difference between features: which will be the update step\n",
    "        return fvector\n",
    "\n",
    "    return subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data, parameters, feature_names, tagset, score_func, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates precision, recall, and F1 of the tagger compared to the gold standard in the data\n",
    "    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n",
    "        make_data_point function)\n",
    "    :param parameters: FeatureVector.  The model parameters\n",
    "    :param feature_names: Array of Strings.  The list of features.\n",
    "    :param tagset: Array of Strings.  The list of tags.\n",
    "    :return: Tuple of (prec, rec, f1)\n",
    "    \"\"\"\n",
    "    all_gold_tags = []\n",
    "    all_predicted_tags = []\n",
    "    for inputs in tqdm(data, desc=\"Evaluating...\", colour=\"green\"):\n",
    "        all_gold_tags.extend(\n",
    "            inputs[\"gold_tags\"][1:-1]\n",
    "        )  # deletes <START> and <STOP>\n",
    "        input_len = len(inputs[\"tokens\"])\n",
    "        all_predicted_tags.extend(\n",
    "            predict(\n",
    "                inputs,\n",
    "                input_len,\n",
    "                parameters,\n",
    "                feature_names,\n",
    "                tagset,\n",
    "                score_func,\n",
    "            )[1:-1]\n",
    "        )  # deletes <START> and <STOP>\n",
    "    return conllevaluate(all_gold_tags, all_predicted_tags, verbose)\n",
    "\n",
    "\n",
    "def write_reports(reports, filename, columns):\n",
    "    report_map = {}\n",
    "    for i, column in enumerate(columns):\n",
    "        values = []\n",
    "        for report in reports:\n",
    "            value = report[i]\n",
    "            values.append(value)\n",
    "        report_map[column] = values\n",
    "\n",
    "    report_df = pd.DataFrame(data=report_map)\n",
    "    report_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data,\n",
    "    feature_names,\n",
    "    tagset,\n",
    "    epochs,\n",
    "    optimizer,\n",
    "    score_func=perceptron_score,\n",
    "    step_size=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model on the data and returns the parameters\n",
    "    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n",
    "        make_data_point function).\n",
    "    :param feature_names: Array of Strings.  The list of feature names.\n",
    "    :param tagset: Array of Strings.  The list of tags.\n",
    "    :param epochs: Int. The number of epochs to train\n",
    "    :return: FeatureVector. The learned parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = FeatureVector({})  # creates a zero vector\n",
    "    gradient = get_gradient(\n",
    "        data, feature_names, tagset, parameters, score_func\n",
    "    )\n",
    "\n",
    "    def training_observer(epoch, parameters):\n",
    "        \"\"\"\n",
    "        Evaluates the parameters on the development data, and writes out the parameters to a 'model.iter'+epoch and\n",
    "        the predictions to 'ner.dev.out'+epoch.\n",
    "        :param epoch: int.  The epoch\n",
    "        :param parameters: Feature Vector.  The current parameters\n",
    "        :return: Double. F1 on the development data\n",
    "        \"\"\"\n",
    "        (_, _, f1) = evaluate(\n",
    "            dev_data, parameters, feature_names, tagset, score_func\n",
    "        )\n",
    "\n",
    "        return f1\n",
    "\n",
    "    # return the final parameters\n",
    "    return optimizer(\n",
    "        sample_num,\n",
    "        epochs,\n",
    "        gradient,\n",
    "        parameters,\n",
    "        training_observer,\n",
    "        step_size=step_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dev_and_test(parameters, feature_names, score, name):\n",
    "    report_cols = [\"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "    # generating dev report\n",
    "    report = evaluate(dev_data, parameters, feature_names, tagset, score)\n",
    "    dev_precision, dev_recall, dev_f1 = report\n",
    "    print(\n",
    "        f\"\\nDev=> Precision: {dev_precision} Recall: {dev_recall} F-1: {dev_f1}\"\n",
    "    )\n",
    "    write_predictions(\n",
    "        f\"{name}.dev.pred\",\n",
    "        dev_data,\n",
    "        parameters,\n",
    "        feature_names,\n",
    "        tagset,\n",
    "        score,\n",
    "    )\n",
    "    write_reports(\n",
    "        [list(report)],\n",
    "        f\"{name}.dev.report\",\n",
    "        report_cols,\n",
    "    )\n",
    "\n",
    "    # generating test report\n",
    "    report = evaluate(test_data, parameters, feature_names, tagset, score)\n",
    "    test_precision, test_recall, test_f1 = report\n",
    "    print(\n",
    "        f\"Test=> Precision: {test_precision} Recall: {test_recall} F-1: {test_f1}\\n\"\n",
    "    )\n",
    "    write_predictions(\n",
    "        f\"{name}.test.pred\",\n",
    "        test_data,\n",
    "        parameters,\n",
    "        feature_names,\n",
    "        tagset,\n",
    "        score,\n",
    "    )\n",
    "    write_reports(\n",
    "        [list(report)],\n",
    "        f\"{name}.test.report\",\n",
    "        report_cols,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_experiment(feature_names, name, optimizer, write_params=False):\n",
    "    print(f\"=============Results of Experiment {name}============\")\n",
    "    parameters = train(\n",
    "        train_data,\n",
    "        feature_names,\n",
    "        tagset,\n",
    "        epochs=5,\n",
    "        score_func=perceptron_score,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    evaluate_dev_and_test(\n",
    "        parameters, feature_names, perceptron_score, name=name\n",
    "    )\n",
    "    if write_params:\n",
    "        # write parameters\n",
    "        parameters.write_to_file(f\"{name}.parameters\")\n",
    "\n",
    "\n",
    "def svm_experiment(cost, feature_names, name):\n",
    "    print(f\"\\n\\n=============Results of Experiment {name}============\\n\\n\")\n",
    "    step_sizes = [10, 50, 100]\n",
    "    l2_lambdas = [0.0001, 0.0005, 0.001]\n",
    "    tunning_columns = [\"step_size\", \"l2_lambda\", \"precision\", \"recall\", \"f1\"]\n",
    "    score = svm_with_cost_func(cost)\n",
    "    max_f1 = float(\"-inf\")\n",
    "    best_parameters = FeatureVector({})\n",
    "    reports = []\n",
    "    for step_size in step_sizes:\n",
    "        for l2_lambda in l2_lambdas:\n",
    "            print(f\"step: {step_size}, lambda: {l2_lambda}\")\n",
    "            parameters = train(\n",
    "                train_data,\n",
    "                feature_names,\n",
    "                tagset,\n",
    "                epochs=10,\n",
    "                step_size=step_size,\n",
    "                score_func=score,\n",
    "                optimizer=optimizer(\n",
    "                    update_func=\"l2_regularizer\", l2_lambda=l2_lambda\n",
    "                ),\n",
    "            )\n",
    "            precision, recall, f1 = evaluate(\n",
    "                dev_data, parameters, feature_names, tagset, score\n",
    "            )\n",
    "            print(\n",
    "                f\"Tuning=> Precision: {precision} Recall: {recall} F-1: {f1}\\n\"\n",
    "            )\n",
    "            reports.append([step_size, l2_lambda, precision, recall, f1])\n",
    "            if f1 > max_f1:\n",
    "                tuned_step_size = step_size\n",
    "                tuned_l2_lambda = l2_lambda\n",
    "                best_parameters = FeatureVector({})\n",
    "                best_parameters.times_plus_equal(1, parameters)\n",
    "                max_f1 = f1\n",
    "    print(f\"\\nBest!! step: {tuned_step_size}, lambda: {tuned_l2_lambda}\\n\")\n",
    "    write_reports(reports, f\"{name}.tuning.report\", tunning_columns)\n",
    "    evaluate_dev_and_test(best_parameters, feature_full, score, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    # feat1-4 perceptron ssgd\n",
    "    perceptron_experiment(\n",
    "        feature_names=feature_1_to_4,\n",
    "        name=\"feat1-4_perceptron_ssgd\",\n",
    "        optimizer=optimizer(),\n",
    "    )\n",
    "\n",
    "    # feat-full perceptron ssgd\n",
    "    perceptron_experiment(\n",
    "        feature_names=feature_full,\n",
    "        name=\"featfull_perceptron_ssgd\",\n",
    "        optimizer=optimizer(),\n",
    "        write_params=True,\n",
    "    )\n",
    "\n",
    "    # feat-full perceptron adagrad\n",
    "    perceptron_experiment(\n",
    "        feature_names=feature_full,\n",
    "        name=\"featfull_perceptron_adagrad\",\n",
    "        optimizer=optimizer(update_func=\"adagrad\"),\n",
    "    )\n",
    "\n",
    "    # feat-full svm ssgd: tune step_size and regularizer\n",
    "    svm_experiment(\n",
    "        hamming_loss(), feature_names=feature_full, name=\"feat-full_svm_ssgd\"\n",
    "    )\n",
    "\n",
    "    # feat-full modified-svm ssgd: tune step_size and regularizer\n",
    "    svm_experiment(\n",
    "        hamming_loss(penalty=30),\n",
    "        feature_names=feature_full,\n",
    "        name=\"feat-full_modified-svm_ssgd\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Results of Experiment feat1-4_perceptron_ssgd============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:02<00:00, 198.34it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [00:00<00:00, 204.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 1: 19.337016574585633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:02<00:00, 204.29it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [00:00<00:00, 203.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 2: 38.476190476190474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:02<00:00, 205.03it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [00:00<00:00, 204.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 3: 30.501930501930502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:02<00:00, 203.76it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [00:00<00:00, 204.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 4: 46.728971962616825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:02<00:00, 202.22it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [00:00<00:00, 205.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 5: 27.079303675048354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [00:00<00:00, 206.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev=> Precision: 45.62043795620438 Recall: 47.89272030651341 F-1: 46.728971962616825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [00:00<00:00, 236.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test=> Precision: 54.78260869565217 Recall: 55.021834061135365 F-1: 54.90196078431373\n",
      "\n",
      "=============Results of Experiment featfull_perceptron_ssgd============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [04:23<00:00,  1.90it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [01:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 1: 52.80289330922243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [04:30<00:00,  1.85it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [01:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 2: 57.19489981785063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [04:22<00:00,  1.91it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [01:16<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 3: 55.51330798479087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [04:21<00:00,  1.91it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [01:17<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 4: 51.34649910233393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [04:23<00:00,  1.90it/s]\n",
      "Evaluating: 100%|██████████| 150/150 [01:17<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score at epoch 5: 55.91836734693878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [01:15<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev=> Precision: 54.513888888888886 Recall: 60.15325670498084 F-1: 57.19489981785063\n"
     ]
    }
   ],
   "source": [
    "experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
